{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Preprocessing - Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the complete dataset\n",
    "rental_df_landing = pd.read_csv('../../data/landing/rental_df_landing.csv')\n",
    "\n",
    "rental_df_landing = rental_df_landing.rename(columns={'distance/ç±³': 'distance/m'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply range check for bedroom, bathroom, and parking on the complete dataset\n",
    "rental_df_landing['bedroom'] = rental_df_landing['bedroom'].apply(lambda x: x if 1 <= x <= 6 else np.nan)\n",
    "rental_df_landing['bathroom'] = rental_df_landing['bathroom'].apply(lambda x: x if 1 <= x <= 8 else np.nan)\n",
    "rental_df_landing['parking'] = rental_df_landing['parking'].apply(lambda x: x if 0 <= x <= 5 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets containing the education index\n",
    "train_df_schoolRelated_w_educationIndex = pd.read_csv('../../data/curated/train_df_schoolRelated_curated.csv')\n",
    "test_df_schoolRelated_w_educationIndex = pd.read_csv('../../data/curated/test_df_schoolRelated_curated.csv')\n",
    "\n",
    "# Separate the train and test datasets from rental_df_landing based on 'id'\n",
    "rental_df_landing_train = rental_df_landing[rental_df_landing['id'].isin(train_df_schoolRelated_w_educationIndex['id'])]\n",
    "rental_df_landing_test = rental_df_landing[rental_df_landing['id'].isin(test_df_schoolRelated_w_educationIndex['id'])]\n",
    "\n",
    "# Perform left merge to include the education index in the train dataset\n",
    "rental_df_landing_train = pd.merge(\n",
    "    rental_df_landing_train, \n",
    "    train_df_schoolRelated_w_educationIndex[['id', 'educationIndex']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Perform left merge to include the education index in the test dataset\n",
    "rental_df_landing_test = pd.merge(\n",
    "    rental_df_landing_test, \n",
    "    test_df_schoolRelated_w_educationIndex[['id', 'educationIndex']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Replace the 'rent' column in the test dataset with values from test_df_schoolRelated_w_educationIndex\n",
    "rental_df_landing_train['rent'] = rental_df_landing_train['id'].map(\n",
    "    train_df_schoolRelated_w_educationIndex.set_index('id')['rent']\n",
    ")\n",
    "\n",
    "rental_df_landing_test['rent'] = rental_df_landing_test['id'].map(\n",
    "    test_df_schoolRelated_w_educationIndex.set_index('id')['rent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the school-related columns to drop\n",
    "school_related_columns_to_drop = ['educationLevel', 'name', 'distance/m', 'year', 'gender', 'get_type']\n",
    "\n",
    "# Drop school-related columns from the train dataset\n",
    "rental_df_landing_train = rental_df_landing_train.drop(columns=school_related_columns_to_drop)\n",
    "\n",
    "rental_df_landing_test = rental_df_landing_test.drop(columns=school_related_columns_to_drop)\n",
    "\n",
    "# Remove duplicates based on the 'id' column (keep the first occurrence) in the train dataset\n",
    "rental_df_landing_train_no_duplicates = rental_df_landing_train.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "rental_df_landing_test_no_duplicates = rental_df_landing_test.drop_duplicates(subset=['id'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_train_clean_represent = rental_df_landing_train_no_duplicates.copy()\n",
    "rental_df_test_clean_represent = rental_df_landing_test_no_duplicates.copy()\n",
    "\n",
    "rental_df_train_clean_represent['land'] = rental_df_train_clean_represent['land'].str.extract(r'^(\\d+)').astype(float)\n",
    "\n",
    "# Define the minimum and maximum allowed land values\n",
    "min_allowed_land = 10  # Example minimum allowed value\n",
    "max_allowed_land = 10000  # Example maximum allowed value\n",
    "\n",
    "# Replace values outside the allowed range with NaN in the train dataset\n",
    "rental_df_train_clean_represent['land'] = rental_df_train_clean_represent['land'].apply(lambda x: x if min_allowed_land <= x <= max_allowed_land else np.nan)\n",
    "\n",
    "mean_land_value_train = rental_df_train_clean_represent['land'].mean()\n",
    "\n",
    "# Fill missing values in the train dataset with the mean\n",
    "rental_df_train_clean_represent['land'].fillna(mean_land_value_train, inplace=True)\n",
    "\n",
    "rental_df_test_clean_represent['land'] = rental_df_test_clean_represent['land'].str.extract(r'^(\\d+)').astype(float)\n",
    "\n",
    "# Replace values outside the allowed range with NaN in the test dataset\n",
    "rental_df_test_clean_represent['land'] = rental_df_test_clean_represent['land'].apply(lambda x: x if min_allowed_land <= x <= max_allowed_land else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage columns\n",
    "percentage_columns = ['under 20', '20-39', '40-59', '60+', 'Owner', 'Renter', 'Family', 'Single']\n",
    "\n",
    "# Step 2: Clean percentage columns for the train dataset (remove '%', convert to int)\n",
    "rental_df_train_clean_represent[percentage_columns] = rental_df_train_clean_represent[percentage_columns].apply(\n",
    "    lambda x: x.str.replace('%', '').astype(int)\n",
    ")\n",
    "\n",
    "# Step 2: Clean percentage columns for the test dataset (remove '%', convert to int)\n",
    "rental_df_test_clean_represent[percentage_columns] = rental_df_test_clean_represent[percentage_columns].apply(\n",
    "    lambda x: x.str.replace('%', '').astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique property types in the train dataset BEFORE standardizing:\n",
      "['House' 'Terrace' 'Villa' 'Semi-Detached' 'New House & Land' 'Duplex'\n",
      " 'Apartment / Unit / Flat' 'Studio' 'New Apartments / Off the Plan'\n",
      " 'Block of Units' 'Townhouse']\n",
      "\n",
      "Unique property types in the train dataset AFTER standardizing:\n",
      "['House' 'Terrace' 'Villa' 'Semi-Detached' 'Duplex' 'Apartment'\n",
      " 'Block of Units' 'Townhouse']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique property types in the train dataset BEFORE standardizing:\")\n",
    "print(rental_df_train_clean_represent['propertyType'].unique())\n",
    "\n",
    "# Simplifying and standardizing the property types for the train dataset\n",
    "rental_df_train_clean_represent['propertyType'] = rental_df_train_clean_represent['propertyType'].replace({\n",
    "    'New House & Land': 'House', \n",
    "    'Apartment / Unit / Flat': 'Apartment',\n",
    "    'Studio': 'Apartment',\n",
    "    'New Apartments / Off the Plan': 'Apartment',\n",
    "    # Add more replacements if necessary\n",
    "})\n",
    "\n",
    "rental_df_test_clean_represent['propertyType'] = rental_df_test_clean_represent['propertyType'].replace({\n",
    "    'New House & Land': 'House', \n",
    "    'Apartment / Unit / Flat': 'Apartment',\n",
    "    'Studio': 'Apartment',\n",
    "    'New Apartments / Off the Plan': 'Apartment',\n",
    "    # Add more replacements if necessary\n",
    "})\n",
    "\n",
    "print(\"\\nUnique property types in the train dataset AFTER standardizing:\")\n",
    "print(rental_df_train_clean_represent['propertyType'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_train_clean_represent['property'] = rental_df_train_clean_represent['property'].apply(\n",
    "    lambda x: ', '.join(sorted(x.split(', '))) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "rental_df_test_clean_represent['property'] = rental_df_test_clean_represent['property'].apply(\n",
    "    lambda x: ', '.join(sorted(x.split(', '))) if pd.notna(x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_train_nomiss = rental_df_train_clean_represent.copy()\n",
    "rental_df_test_nomiss = rental_df_test_clean_represent.copy()\n",
    "\n",
    "# Step 2: Convert bedroom, bathroom, and parking columns to integers if not NaN for the train dataset\n",
    "rental_df_train_nomiss['bedroom'] = rental_df_train_nomiss['bedroom'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "rental_df_train_nomiss['bathroom'] = rental_df_train_nomiss['bathroom'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "rental_df_train_nomiss['parking'] = rental_df_train_nomiss['parking'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Fill missing values in the train dataset with the mode\n",
    "bedroom_mode_train = rental_df_train_nomiss['bedroom'].mode()[0]\n",
    "bathroom_mode_train = rental_df_train_nomiss['bathroom'].mode()[0]\n",
    "parking_mode_train = rental_df_train_nomiss['parking'].mode()[0]\n",
    "\n",
    "rental_df_train_nomiss['bedroom'].fillna(bedroom_mode_train, inplace=True)\n",
    "rental_df_train_nomiss['bathroom'].fillna(bathroom_mode_train, inplace=True)\n",
    "rental_df_train_nomiss['parking'].fillna(parking_mode_train, inplace=True)\n",
    "\n",
    "# Step 3: Convert bedroom, bathroom, and parking columns to integers if not NaN for the test dataset\n",
    "rental_df_test_nomiss['bedroom'] = rental_df_test_nomiss['bedroom'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "rental_df_test_nomiss['bathroom'] = rental_df_test_nomiss['bathroom'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "rental_df_test_nomiss['parking'] = rental_df_test_nomiss['parking'].apply(lambda x: int(x) if not pd.isna(x) else x)\n",
    "\n",
    "# Fill missing values in the test dataset using the mode from the train dataset\n",
    "rental_df_test_nomiss['bedroom'].fillna(bedroom_mode_train, inplace=True)\n",
    "rental_df_test_nomiss['bathroom'].fillna(bathroom_mode_train, inplace=True)\n",
    "rental_df_test_nomiss['parking'].fillna(parking_mode_train, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean land value from the train dataset\n",
    "mean_land_value_train = rental_df_train_nomiss['land'].mean()\n",
    "\n",
    "# Fill missing values in the train dataset with the mean\n",
    "rental_df_train_nomiss['land'].fillna(mean_land_value_train, inplace=True)\n",
    "\n",
    "rental_df_test_nomiss['land'].fillna(mean_land_value_train, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'property' column contains multiple labels, so missing values are filled with 'Unknown' for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_train_nomiss['property'] = rental_df_train_nomiss['property'].fillna('Unknown')\n",
    "\n",
    "rental_df_test_nomiss['property'] = rental_df_test_nomiss['property'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_train_nomiss = rental_df_train_nomiss.rename(columns={\n",
    "    'property': 'propertyFeatures'\n",
    "})\n",
    "\n",
    "rental_df_test_nomiss = rental_df_test_nomiss.rename(columns={\n",
    "    'property': 'propertyFeatures'\n",
    "})\n",
    "\n",
    "# Define columns to drop and apply it to both the train and test datasets\n",
    "columns_to_drop = ['_url', 'page', 'feature', 'type', 'Available', 'Bond']\n",
    "\n",
    "rental_df_train_nomiss = rental_df_train_nomiss.drop(columns=columns_to_drop, errors='ignore')\n",
    "rental_df_test_nomiss = rental_df_test_nomiss.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Save the cleaned train and test datasets to CSV files\n",
    "rental_df_train_nomiss.to_csv('../../data/raw/rental_df_train_raw.csv', index=False)\n",
    "rental_df_test_nomiss.to_csv('../../data/raw/rental_df_test_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curated Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Part 1) Feature Engineering on address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suburb (mean encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract suburb from the 'address' column and insert it after 'address' in the train dataset\n",
    "rental_df_train_nomiss['suburb'] = rental_df_train_nomiss['address'].str.extract(r'(\\d{4})$')\n",
    "\n",
    "# Find the index of the 'address' column in the train dataset\n",
    "address_idx_train = rental_df_train_nomiss.columns.get_loc('address')\n",
    "\n",
    "# Insert 'suburb' column next to 'address'\n",
    "rental_df_train_nomiss.insert(address_idx_train + 1, 'suburb', rental_df_train_nomiss.pop('suburb'))\n",
    "\n",
    "# Step 2: Extract suburb from the 'address' column and insert it after 'address' in the test dataset\n",
    "rental_df_test_nomiss['suburb'] = rental_df_test_nomiss['address'].str.extract(r'(\\d{4})$')\n",
    "\n",
    "# Find the index of the 'address' column in the test dataset\n",
    "address_idx_test = rental_df_test_nomiss.columns.get_loc('address')\n",
    "\n",
    "# Insert 'suburb' column next to 'address'\n",
    "rental_df_test_nomiss.insert(address_idx_test + 1, 'suburb', rental_df_test_nomiss.pop('suburb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean rent by suburb in the train dataset\n",
    "suburb_mean_rent_train = rental_df_train_nomiss.groupby('suburb')['rent'].mean()\n",
    "\n",
    "# Map the means to the 'suburb' column in both train and test datasets\n",
    "rental_df_train_nomiss['suburb_encoded'] = rental_df_train_nomiss['suburb'].map(suburb_mean_rent_train)\n",
    "rental_df_test_nomiss['suburb_encoded'] = rental_df_test_nomiss['suburb'].map(suburb_mean_rent_train)\n",
    "\n",
    "# Fill NaN values in the test set (if a suburb is not in the train set) with the overall mean rent\n",
    "overall_mean_rent = rental_df_train_nomiss['rent'].mean()\n",
    "rental_df_test_nomiss['suburb_encoded'].fillna(overall_mean_rent, inplace=True)\n",
    "\n",
    "# Step 1: Get the column index of 'suburb'\n",
    "suburb_index_train = rental_df_train_nomiss.columns.get_loc('suburb')\n",
    "suburb_index_test = rental_df_test_nomiss.columns.get_loc('suburb')\n",
    "\n",
    "# Step 2: Drop the original 'suburb' column (for now)\n",
    "rental_df_train_nomiss = rental_df_train_nomiss.drop(columns=['suburb'])\n",
    "rental_df_test_nomiss = rental_df_test_nomiss.drop(columns=['suburb'])\n",
    "\n",
    "# Step 3: Insert 'suburb_encoded' into the original position of 'suburb'\n",
    "rental_df_train_nomiss.insert(suburb_index_train, 'suburb_encoded', rental_df_train_nomiss.pop('suburb_encoded'))\n",
    "rental_df_test_nomiss.insert(suburb_index_test, 'suburb_encoded', rental_df_test_nomiss.pop('suburb_encoded'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transport Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each station serves routes in both directions, we should treat them as a single station when calculating the transport index for a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanjunqi/anaconda3/lib/python3.11/site-packages/pyogrio/geopandas.py:261: UserWarning: More than one layer found in '': 'PTV_METRO_BUS_STOP' (default), 'PTV_METRO_TRAIN_STATION', 'PTV_TRAIN_CORRIDOR_CENTRELINE', 'PTV_TRAIN_STATION_BIKE_STORAGE', 'PTV_METRO_BUS_ROUTE', 'PTV_TRAIN_TRACK_CENTRELINE', 'PTV_METRO_TRAM_ROUTE', 'PTV_METRO_TRAM_STOP', 'PTV_REGIONAL_BUS_ROUTE', 'PTV_TRAM_TRACK_CENTRELINE', 'PTV_TRAIN_STATION_PLATFORM', 'PTV_REGIONAL_COACH_STOP', 'PTV_REGIONAL_TRAIN_STATION', 'PTV_REGIONAL_COACH_ROUTE', 'PTV_REGIONAL_BUS_STOP', 'PTV_SKYBUS_ROUTE', 'PTV_SKYBUS_STOP', 'PTV_TRAIN_CARPARK'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load bus stop data\n",
    "bus_gdf_path = '../../data/external/BUS.gdb/'\n",
    "bus_gdf = gpd.read_file(bus_gdf_path)\n",
    "\n",
    "# Step 2: Find duplicate stops based on 'STOP_NAME'\n",
    "duplicate_stops = bus_gdf[bus_gdf.duplicated(subset='STOP_NAME', keep=False)]\n",
    "\n",
    "# Step 3: Calculate the number of routes for each stop\n",
    "bus_gdf['num_routes'] = bus_gdf['ROUTES_USING_STOP'].apply(lambda x: len(str(x).split(',')))  # Count routes per stop\n",
    "\n",
    "# Step 4: Sort by 'STOP_NAME' and 'num_routes' to prioritize stops with more routes\n",
    "bus_gdf_sorted = bus_gdf.sort_values(by=['STOP_NAME', 'num_routes'], ascending=[True, False])\n",
    "\n",
    "# Step 5: Drop duplicate stops, keeping the one with the most routes\n",
    "bus_gdf_cleaned = bus_gdf_sorted.drop_duplicates(subset='STOP_NAME', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load tram stop data\n",
    "tram_gdf_path = '../../data/external/TRAM/'\n",
    "tram_gdf = gpd.read_file(tram_gdf_path)\n",
    "\n",
    "# Step 2: Find duplicate stops based on 'STOP_NAME'\n",
    "duplicate_stops = tram_gdf[tram_gdf.duplicated(subset='STOP_NAME', keep=False)]\n",
    "\n",
    "# Step 3: Calculate the number of routes for each tram stop\n",
    "tram_gdf['num_routes'] = tram_gdf['ROUTEUSSP'].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "# Step 4: Sort by 'STOP_NAME' and 'num_routes' to prioritize stops with more routes\n",
    "tram_gdf_sorted = tram_gdf.sort_values(by=['STOP_NAME', 'num_routes'], ascending=[True, False])\n",
    "\n",
    "# Step 5: Drop duplicate stops, keeping the one with the most routes\n",
    "tram_gdf_cleaned = tram_gdf_sorted.drop_duplicates(subset='STOP_NAME', keep='first').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf_path = '../../data/external/TRAIN/'\n",
    "train_gdf = gpd.read_file(train_gdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus routes across CBD: {'237', '232', '350', '303', '309', '216', '220', '234', '235', '236', '250', '200', '251', '207'}\n",
      "Tram routes across CBD: {'96', '11', '35', '48', '109', '59', '75', '70', '57', '12', '19', '58'}\n"
     ]
    }
   ],
   "source": [
    "tram_gdf_cleaned.rename(columns={'ROUTEUSSP': 'ROUTES_USING_STOP'}, inplace=True)\n",
    "\n",
    "\n",
    "# Define Melbourne CBD coordinates and create the polygon\n",
    "# Coordinates are defined in EPSG:4326\n",
    "melbourne_cbd_coords = [\n",
    "    (144.946457, -37.819722),  # North-West corner (near Flagstaff)\n",
    "    (144.9785, -37.810087),    # North-East corner (near Parliament)\n",
    "    (144.9609, -37.818257),    # South-East corner (near Southbank)\n",
    "    (144.946457, -37.824722),  # South-West corner (near Southern Cross)\n",
    "]\n",
    "melbourne_cbd_polygon = Polygon(melbourne_cbd_coords)\n",
    "\n",
    "# Create a GeoDataFrame for the CBD\n",
    "# The CBD polygon is created in EPSG:4326 (WGS84), so we set that CRS here\n",
    "melbourne_cbd_gdf = gpd.GeoDataFrame(index=[0], crs='EPSG:4326', geometry=[melbourne_cbd_polygon])\n",
    "\n",
    "# Function to standardize and filter stops within the CBD\n",
    "def filter_stops_within_cbd(stops_gdf, cbd_polygon):\n",
    "    # Ensure that stops are reprojected to the same CRS as the CBD polygon (EPSG:4326)\n",
    "    if stops_gdf.crs != 'EPSG:4326':\n",
    "        stops_gdf = stops_gdf.to_crs('EPSG:4326')\n",
    "    # Filter stops within the CBD polygon\n",
    "    stops_in_cbd = stops_gdf[stops_gdf['geometry'].within(cbd_polygon)]\n",
    "    return stops_in_cbd\n",
    "\n",
    "# Load and filter bus and tram stops within the CBD\n",
    "gdf_bus_stops_in_cbd = filter_stops_within_cbd(bus_gdf_cleaned, melbourne_cbd_polygon)\n",
    "gdf_tram_stops_in_cbd = filter_stops_within_cbd(tram_gdf_cleaned, melbourne_cbd_polygon)\n",
    "\n",
    "# Extract unique routes for bus and tram stops within the CBD\n",
    "bus_routes_across_cbd = set(','.join(gdf_bus_stops_in_cbd['ROUTES_USING_STOP'].unique()).split(','))\n",
    "tram_routes_across_cbd = set(','.join(gdf_tram_stops_in_cbd['ROUTES_USING_STOP'].unique()).split(','))\n",
    "\n",
    "# Step 6: Print the results\n",
    "print(f\"Bus routes across CBD: {bus_routes_across_cbd}\")\n",
    "print(f\"Tram routes across CBD: {tram_routes_across_cbd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_bus_route_across_cbd(routes):\n",
    "    stop_routes = set(routes.split(','))\n",
    "    return bool(stop_routes & bus_routes_across_cbd)\n",
    "\n",
    "def has_tram_route_across_cbd(routes):\n",
    "    stop_routes = set(routes.split(','))\n",
    "    return bool(stop_routes & tram_routes_across_cbd)\n",
    "\n",
    "def flag_stations(row):\n",
    "    if row['HAS_ROUTE_ACROSS_CBD']:  # Already flagged as inside CBD\n",
    "        return True\n",
    "    elif row['STOP_ZONE'] and '1' in row['STOP_ZONE']:  # Likely connected to CBD (Zone 1) and non-None\n",
    "        return True\n",
    "    else:\n",
    "        return np.nan  # Fill with NaN for now, to be handled later\n",
    "\n",
    "# Apply the relevant function to bus and tram stops\n",
    "bus_gdf_cleaned['HAS_ROUTE_ACROSS_CBD'] = bus_gdf_cleaned['ROUTES_USING_STOP'].apply(has_bus_route_across_cbd)\n",
    "tram_gdf_cleaned['HAS_ROUTE_ACROSS_CBD'] = tram_gdf_cleaned['ROUTES_USING_STOP'].apply(has_tram_route_across_cbd)\n",
    "\n",
    "train_gdf['HAS_ROUTE_ACROSS_CBD'] = train_gdf['geometry'].within(melbourne_cbd_polygon)\n",
    "\n",
    "# Combine the data from all transportation modes temporarily to calculate the mode\n",
    "combined_transport_gdf = pd.concat([bus_gdf_cleaned[['HAS_ROUTE_ACROSS_CBD']], \n",
    "                                    tram_gdf_cleaned[['HAS_ROUTE_ACROSS_CBD']], \n",
    "                                    train_gdf[['HAS_ROUTE_ACROSS_CBD']]])\n",
    "\n",
    "# Get the mode of 'HAS_ROUTE_ACROSS_CBD' across all transportation modes, excluding NaN\n",
    "has_route_mode_all_transport = combined_transport_gdf['HAS_ROUTE_ACROSS_CBD'].dropna().mode()[0]\n",
    "\n",
    "# Apply the updated function to the train data\n",
    "train_gdf['HAS_ROUTE_ACROSS_CBD'] = train_gdf.apply(flag_stations, axis=1)\n",
    "\n",
    "# Now fill missing values in the train data using the mode from the combined dataset\n",
    "train_gdf['HAS_ROUTE_ACROSS_CBD'].fillna(has_route_mode_all_transport, inplace=True)\n",
    "\n",
    "# Step 5: Fill missing values in the 'HAS_ROUTE_ACROSS_CBD' column for the train data using the mode\n",
    "train_gdf['HAS_ROUTE_ACROSS_CBD'].fillna(has_route_mode_all_transport, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Caculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first need to find geometry of each property's address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Nominatim geocoder with a custom user agent\n",
    "geolocator = Nominatim(user_agent=\"junqis\")\n",
    "\n",
    "# Function to geocode an address with a delay and handle errors\n",
    "def geocode_address_with_delay(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address, timeout=10)  # Increased timeout\n",
    "        time.sleep(1)  # Delay of 1 second to avoid rate limits\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {address}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 1: Load or prepare train and test datasets\n",
    "train_df = rental_df_train_nomiss.copy()\n",
    "test_df = rental_df_test_nomiss.copy()\n",
    "\n",
    "# Step 2: Concatenate train and test datasets into one combined dataframe\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Initialize latitude and longitude columns as None if they do not exist\n",
    "combined_df[['latitude', 'longitude']] = None\n",
    "\n",
    "# Step 3: Iterate over rows and geocode addresses for the combined dataframe\n",
    "for idx, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    if pd.isna(row['latitude']) or pd.isna(row['longitude']):  # Geocode only if lat/lon is missing\n",
    "        lat, lon = geocode_address_with_delay(row['address'])\n",
    "        combined_df.at[idx, 'latitude'] = lat\n",
    "        combined_df.at[idx, 'longitude'] = lon\n",
    "\n",
    "# combined_df.to_csv('../../data/distance/rental_df_w_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_w_geometry = pd.read_csv('../../data/distance/rental_df_w_geometry.csv')\n",
    "\n",
    "rental_df_w_geometry['geometry'] = rental_df_w_geometry.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the geometry is determined, we can then find the nearest transportation stops from the properties using straight-line distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the n nearest stations for all properties in one go (vectorized)\n",
    "def find_nearest_stations(property_gdf, stops_gdf, n=5, transport_type='Bus'):\n",
    "    expanded_data = []\n",
    "    \n",
    "    # For each property, calculate the distance to all stops at once (vectorized)\n",
    "    for idx, property_row in property_gdf.iterrows():\n",
    "        property_geom = property_row['geometry']\n",
    "        \n",
    "        # Calculate distances to all stops at once (vectorized operation)\n",
    "        stops_gdf['distance'] = stops_gdf['geometry'].distance(property_geom)\n",
    "        \n",
    "        # Filter out invalid distances (NaN or inf)\n",
    "        valid_stops = stops_gdf[stops_gdf['distance'].notna() & (stops_gdf['distance'] != float('inf'))]\n",
    "        \n",
    "        # Sort the stops by distance and take the top n nearest stops\n",
    "        nearest_stops = valid_stops.nsmallest(n, 'distance')\n",
    "        \n",
    "        # Fill in missing stops with None if fewer than n stops are found\n",
    "        for i in range(n):\n",
    "            if i < len(nearest_stops):\n",
    "                stop_row = nearest_stops.iloc[i]\n",
    "                expanded_data.append({\n",
    "                    'id': property_row['id'],\n",
    "                    'rent': property_row['rent'],\n",
    "                    'address': property_row['address'],\n",
    "                    'nearest_stop_id': stop_row.get('STOP_ID', None),\n",
    "                    'nearest_stop_name': stop_row.get('STOP_NAME', None),\n",
    "                    'nearest_stop_has_route_across_cbd': stop_row.get('HAS_ROUTE_ACROSS_CBD', None),\n",
    "                    'nearest_stop_distance': stop_row['distance'],\n",
    "                    'transport_type': transport_type  # Include the transportation type\n",
    "                })\n",
    "            else:\n",
    "                # Fill in None for missing stop values if fewer than n stops are found\n",
    "                expanded_data.append({\n",
    "                    'id': property_row['id'],\n",
    "                    'rent': property_row['rent'],\n",
    "                    'address': property_row['address'],\n",
    "                    'nearest_stop_id': None,\n",
    "                    'nearest_stop_name': None,\n",
    "                    'nearest_stop_has_route_across_cbd': None,\n",
    "                    'nearest_stop_distance': None,\n",
    "                    'transport_type': transport_type  # Include the transportation type\n",
    "                })\n",
    "    \n",
    "    return expanded_data\n",
    "\n",
    "train_gdf['STOP_ID'] = range(1, len(train_gdf) + 1)\n",
    "\n",
    "# Prepare GeoDataFrames for bus, tram, and train (already projected to EPSG:3857)\n",
    "property_gdf = gpd.GeoDataFrame(rental_df_w_geometry, geometry='geometry', crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "busStop_gdf = gpd.GeoDataFrame(bus_gdf_cleaned, geometry='geometry', crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "tram_gdf = gpd.GeoDataFrame(tram_gdf_cleaned, geometry='geometry', crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
    "train_gdf = gpd.GeoDataFrame(train_gdf.rename(columns={'STATION': 'STOP_NAME'}), geometry='geometry', crs=\"EPSG:4326\").to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_bus = find_nearest_stations(property_gdf, busStop_gdf, n=5, transport_type='Bus')\n",
    "\n",
    "nearest_tram = find_nearest_stations(property_gdf, tram_gdf, n=5, transport_type='Tram')\n",
    "\n",
    "nearest_train = find_nearest_stations(property_gdf, train_gdf, n=5, transport_type='Train')\n",
    "\n",
    "nearest_stops = pd.DataFrame(nearest_bus + nearest_tram + nearest_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_stops.to_csv('../../data/distance/nearest_stops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the information obtained from the straight-line distance, we can refine the search to find the nearest transportation stops to the properties based on route distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get route distances using Google Distance Matrix API\n",
    "def get_route_distances(origin, destinations, api_key):\n",
    "    destination_string = '|'.join(destinations)  # Join destinations into a string for API request\n",
    "    \n",
    "    url = (\n",
    "        f\"https://maps.googleapis.com/maps/api/distancematrix/json\"\n",
    "        f\"?origins={origin}\"\n",
    "        f\"&destinations={destination_string}\"\n",
    "        f\"&key={api_key}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful (HTTP 200)\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Initialize route distances list\n",
    "        route_distances = []\n",
    "        \n",
    "        # Check if 'rows' exists in the response\n",
    "        if 'rows' in data and 'elements' in data['rows'][0]:\n",
    "            for element in data['rows'][0]['elements']:\n",
    "                if element['status'] == 'OK':\n",
    "                    route_distances.append(element['distance']['value'])  # Distance in meters\n",
    "                else:\n",
    "                    route_distances.append(None)  # Handle cases where distance isn't available\n",
    "        else:\n",
    "            print(\"Warning: No valid distance data returned.\")\n",
    "            route_distances = [None] * len(destinations)  # Fill with None if no valid response\n",
    "        \n",
    "        return route_distances\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any HTTP errors, network errors, etc.\n",
    "        print(f\"Error fetching data from API: {e}\")\n",
    "        return [None] * len(destinations)\n",
    "\n",
    "# Group the data by 'id' and 'address' and aggregate the list of nearest stop names\n",
    "rental_df_perProperty = nearest_stops.groupby(['id', 'address'])['nearest_stop_name'].apply(list).reset_index()\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "to_stop_routeDistances = []\n",
    "\n",
    "# Loop through each property and calculate route distances to its nearest stops\n",
    "for index, property in rental_df_perProperty.iterrows():\n",
    "    property_address = property['address']  # The address of the property\n",
    "    stop_names = property['nearest_stop_name']  # List of nearest stop names\n",
    "\n",
    "    # Get distances using the Google API\n",
    "    distances = get_route_distances(property_address, stop_names, api_key='YOUR_API_KEY_HERE')  # Replace with your API key\n",
    "\n",
    "    # Check if the lengths of stop_names and distances match\n",
    "    if len(distances) != len(stop_names):\n",
    "        print(f\"Warning: Mismatch between stop names and distances for property: {property_address}. Skipping this property.\")\n",
    "        continue  # Skip this property if there's a mismatch\n",
    "\n",
    "    # If distances were successfully calculated, store the results\n",
    "    if distances:\n",
    "        for i, stop in enumerate(stop_names):\n",
    "            distance = {\n",
    "                'id': property['id'],\n",
    "                'address': property_address,\n",
    "                'stop_name': stop,\n",
    "                'route_distance/m': distances[i] if distances[i] is not None else None  # Handle missing distances\n",
    "            }\n",
    "            to_stop_routeDistances.append(distance)\n",
    "\n",
    "# Save the route distances to a CSV file\n",
    "file_path = '../../data/distance/to_stop_routeDistances.csv'\n",
    "to_stop_routeDistances_df = pd.DataFrame(to_stop_routeDistances)\n",
    "to_stop_routeDistances_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_stops = pd.read_csv('../../data/distance/nearest_stops.csv')\n",
    "to_stop_distance = pd.read_csv('../../data/distance/to_stop_routeDistances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on 'id', 'address', and 'stop_name'\n",
    "to_stop_distance = to_stop_distance.drop_duplicates(subset=['id', 'address', 'stop_name'])\n",
    "\n",
    "to_stop_distance.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add an 'order' column to the 'all_transportations' DataFrame to preserve original order after merging\n",
    "nearest_stops['order'] = nearest_stops.index\n",
    "\n",
    "# Step 2: Merge the transport data with the route distance data\n",
    "# Merging on 'id', 'address', and 'nearest_stop_name' from 'all_transportations' \n",
    "# with 'id', 'address', and 'stop_name' from 'to_stop_distance'\n",
    "nearest_stops_w_stop_routeDistances = pd.merge(\n",
    "    nearest_stops, \n",
    "    to_stop_distance[['id', 'address', 'stop_name', 'route_distance/m']], \n",
    "    left_on=['id', 'address', 'nearest_stop_name'],  # Matching 'nearest_stop_name' with 'stop_name'\n",
    "    right_on=['id', 'address', 'stop_name'], \n",
    "    how='left'  # Keeping all records from 'all_transportations'\n",
    ")\n",
    "\n",
    "# Sort the merged DataFrame based on the original 'order' and reset the index\n",
    "nearest_stops_w_stop_routeDistances = nearest_stops_w_stop_routeDistances.sort_values(by='order').reset_index(drop=True)\n",
    "\n",
    "nearest_stops_w_stop_routeDistances = nearest_stops_w_stop_routeDistances.drop(columns=['order'])\n",
    "\n",
    "nearest_stops = nearest_stops.drop(columns=['order'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stops Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = rental_df_train_nomiss['id'].unique()  # Using 'rental_df_train_nomiss' for train IDs\n",
    "test_ids = rental_df_test_nomiss['id'].unique()    \n",
    "\n",
    "# Split transportation data into training and testing sets\n",
    "nearest_stops_train_w_stop_routeDistances = nearest_stops_w_stop_routeDistances[nearest_stops_w_stop_routeDistances['id'].isin(train_ids)]\n",
    "nearest_stops_test_w_stop_routeDistances = nearest_stops_w_stop_routeDistances[nearest_stops_w_stop_routeDistances['id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/3445536539.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_stops_train_w_stop_routeDistances['distance_ratio'] = nearest_stops_train_w_stop_routeDistances.apply(\n",
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/3445536539.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_stops_test_w_stop_routeDistances['distance_ratio'] = nearest_stops_test_w_stop_routeDistances.apply(\n",
      "/Users/shanjunqi/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/shanjunqi/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/3445536539.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_stops_train_w_stop_routeDistances['route_distance/m'] = nearest_stops_train_w_stop_routeDistances.apply(\n",
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/3445536539.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_stops_test_w_stop_routeDistances['route_distance/m'] = nearest_stops_test_w_stop_routeDistances.apply(\n"
     ]
    }
   ],
   "source": [
    "# Calculate 'distance_ratio' for both train and test datasets\n",
    "nearest_stops_train_w_stop_routeDistances['distance_ratio'] = nearest_stops_train_w_stop_routeDistances.apply(\n",
    "    lambda row: row['route_distance/m'] / row['nearest_stop_distance'] \n",
    "    if not pd.isna(row['route_distance/m']) else np.nan, axis=1\n",
    ")\n",
    "\n",
    "nearest_stops_test_w_stop_routeDistances['distance_ratio'] = nearest_stops_test_w_stop_routeDistances.apply(\n",
    "    lambda row: row['route_distance/m'] / row['nearest_stop_distance'] \n",
    "    if not pd.isna(row['route_distance/m']) else np.nan, axis=1\n",
    ")\n",
    "\n",
    "# Apply upper and lower ratio thresholds for both train and test\n",
    "upper_ratio_threshold = 5\n",
    "lower_ratio_threshold = 0.2\n",
    "\n",
    "# For train\n",
    "nearest_stops_train_w_stop_routeDistances.loc[\n",
    "    (nearest_stops_train_w_stop_routeDistances['distance_ratio'] > upper_ratio_threshold) |\n",
    "    (nearest_stops_train_w_stop_routeDistances['distance_ratio'] < lower_ratio_threshold), \n",
    "    ['route_distance/m', 'distance_ratio']] = np.nan\n",
    "\n",
    "# For test\n",
    "nearest_stops_test_w_stop_routeDistances.loc[\n",
    "    (nearest_stops_test_w_stop_routeDistances['distance_ratio'] > upper_ratio_threshold) |\n",
    "    (nearest_stops_test_w_stop_routeDistances['distance_ratio'] < lower_ratio_threshold), \n",
    "    ['route_distance/m', 'distance_ratio']] = np.nan\n",
    "\n",
    "# Calculate property-level and dataset-wide average ratios for the train dataset\n",
    "property_avg_ratios_train = nearest_stops_train_w_stop_routeDistances.groupby('id')['distance_ratio'].apply(\n",
    "    lambda x: np.mean([val for val in x if not pd.isna(val)])\n",
    ")\n",
    "\n",
    "dataset_avg_ratio_train = nearest_stops_train_w_stop_routeDistances['distance_ratio'].mean(skipna=True)\n",
    "\n",
    "# Define function to fill missing route distances\n",
    "def fill_missing_route_distances(row, property_avg_ratios, dataset_avg_ratio):\n",
    "    if pd.isna(row['route_distance/m']):\n",
    "        property_avg_ratio = property_avg_ratios.get(row['id'], np.nan)\n",
    "        if not pd.isna(property_avg_ratio):\n",
    "            return row['nearest_stop_distance'] * property_avg_ratio\n",
    "        else:\n",
    "            return row['nearest_stop_distance'] * dataset_avg_ratio\n",
    "    return row['route_distance/m']\n",
    "\n",
    "# Apply the function to fill missing route distances in both train and test datasets\n",
    "# For train\n",
    "nearest_stops_train_w_stop_routeDistances['route_distance/m'] = nearest_stops_train_w_stop_routeDistances.apply(\n",
    "    fill_missing_route_distances, axis=1, \n",
    "    property_avg_ratios=property_avg_ratios_train, \n",
    "    dataset_avg_ratio=dataset_avg_ratio_train\n",
    ")\n",
    "\n",
    "# For test (using train-derived averages)\n",
    "nearest_stops_test_w_stop_routeDistances['route_distance/m'] = nearest_stops_test_w_stop_routeDistances.apply(\n",
    "    fill_missing_route_distances, axis=1, \n",
    "    property_avg_ratios=property_avg_ratios_train,  # Use property ratios from train\n",
    "    dataset_avg_ratio=dataset_avg_ratio_train  # Use dataset ratio from train\n",
    ")\n",
    "\n",
    "nearest_stops_train_w_stop_routeDistances = nearest_stops_train_w_stop_routeDistances.drop(columns=['distance_ratio'])\n",
    "nearest_stops_test_w_stop_routeDistances = nearest_stops_test_w_stop_routeDistances.drop(columns=['distance_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caculating Transport Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode the transport-related categorical features\n",
    "transport_related_categorical = ['nearest_stop_has_route_across_cbd', 'transport_type']\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "transport_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform on the train dataset\n",
    "transport_encoded_train = transport_encoder.fit_transform(nearest_stops_train_w_stop_routeDistances[transport_related_categorical])\n",
    "\n",
    "# Transform the test dataset using the encoder fitted on the train dataset\n",
    "transport_encoded_test = transport_encoder.transform(nearest_stops_test_w_stop_routeDistances[transport_related_categorical])\n",
    "\n",
    "# Get the encoded feature names\n",
    "encoded_transport_feature_names = transport_encoder.get_feature_names_out(transport_related_categorical)\n",
    "\n",
    "# Create DataFrames for train and test sets with the encoded features\n",
    "# Reset index to ensure alignment when merging the columns back\n",
    "nearest_stops_train_w_stop_routeDistances = nearest_stops_train_w_stop_routeDistances.reset_index(drop=True)\n",
    "nearest_stops_test_w_stop_routeDistances = nearest_stops_test_w_stop_routeDistances.reset_index(drop=True)\n",
    "\n",
    "transport_df_encoded_train = pd.DataFrame(transport_encoded_train, columns=encoded_transport_feature_names)\n",
    "transport_df_encoded_train['id'] = nearest_stops_train_w_stop_routeDistances['id']\n",
    "transport_df_encoded_train['rent'] = nearest_stops_train_w_stop_routeDistances['rent']\n",
    "transport_df_encoded_train['route_distance/m'] = nearest_stops_train_w_stop_routeDistances['route_distance/m']\n",
    "\n",
    "transport_df_encoded_test = pd.DataFrame(transport_encoded_test, columns=encoded_transport_feature_names)\n",
    "transport_df_encoded_test['id'] = nearest_stops_test_w_stop_routeDistances['id']\n",
    "transport_df_encoded_test['rent'] = nearest_stops_test_w_stop_routeDistances['rent']\n",
    "transport_df_encoded_test['route_distance/m'] = nearest_stops_test_w_stop_routeDistances['route_distance/m']\n",
    "\n",
    "# Cross-validation on the train set\n",
    "transport_related_v2 = list(encoded_transport_feature_names)\n",
    "\n",
    "# X and y for the training set\n",
    "X_transport_train = transport_df_encoded_train[transport_related_v2]\n",
    "y_transport_train = transport_df_encoded_train['rent']\n",
    "\n",
    "# Define cross-validation\n",
    "cv_transport = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_model_transport = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validated predictions on the training set\n",
    "transport_score_train = cross_val_predict(rf_model_transport, X_transport_train, y_transport_train, cv=cv_transport)\n",
    "\n",
    "# Step 10: Fit the model on the full training set and predict on the test set\n",
    "rf_model_transport.fit(X_transport_train, y_transport_train)\n",
    "\n",
    "# X for the test set\n",
    "X_transport_test = transport_df_encoded_test[transport_related_v2]\n",
    "\n",
    "# Predict rent for the test set using the trained model\n",
    "transport_score_test = rf_model_transport.predict(X_transport_test)\n",
    "\n",
    "# Step 11: Assign the scores to the respective train and test DataFrames\n",
    "transport_df_encoded_train['transport_score'] = transport_score_train\n",
    "transport_df_encoded_test['transport_score'] = transport_score_test\n",
    "\n",
    "# Normalize the transport scores and distances\n",
    "scaler_transport = MinMaxScaler()\n",
    "\n",
    "# Normalize transport scores for the train set\n",
    "transport_score_reshaped_train = transport_df_encoded_train['transport_score'].values.reshape(-1, 1)\n",
    "transport_score_normalized_train = scaler_transport.fit_transform(transport_score_reshaped_train)\n",
    "transport_df_encoded_train['transport_score'] = transport_score_normalized_train\n",
    "\n",
    "# Normalize transport scores for the test set using the same scaler\n",
    "transport_score_reshaped_test = transport_df_encoded_test['transport_score'].values.reshape(-1, 1)\n",
    "transport_score_normalized_test = scaler_transport.transform(transport_score_reshaped_test)\n",
    "transport_df_encoded_test['transport_score'] = transport_score_normalized_test\n",
    "\n",
    "# Step 13: Normalize reverse distances for both train and test sets\n",
    "distance_scaler_transport = MinMaxScaler()\n",
    "\n",
    "# Train set\n",
    "transport_df_encoded_train['reverse_distance/m'] = 1 - distance_scaler_transport.fit_transform(\n",
    "    transport_df_encoded_train[['route_distance/m']]\n",
    ")\n",
    "\n",
    "# Test set using the scaler fitted on train\n",
    "transport_df_encoded_test['reverse_distance/m'] = 1 - distance_scaler_transport.transform(\n",
    "    transport_df_encoded_test[['route_distance/m']]\n",
    ")\n",
    "\n",
    "# Step 14: Calculate the transportation index for both train and test sets\n",
    "transport_df_encoded_train['transport_index_equally_important_score'] = (\n",
    "    0.5 * transport_df_encoded_train['transport_score'] + 0.5 * transport_df_encoded_train['reverse_distance/m']\n",
    ")\n",
    "\n",
    "transport_df_encoded_test['transport_index_equally_important_score'] = (\n",
    "    0.5 * transport_df_encoded_test['transport_score'] + 0.5 * transport_df_encoded_test['reverse_distance/m']\n",
    ")\n",
    "\n",
    "# Normalize the final transportation index\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize for train set\n",
    "transport_df_encoded_train['transport_index_equally_important_score'] = scaler.fit_transform(\n",
    "    transport_df_encoded_train[['transport_index_equally_important_score']]\n",
    ")\n",
    "\n",
    "# Normalize for test set using the scaler fitted on train\n",
    "transport_df_encoded_test['transport_index_equally_important_score'] = scaler.transform(\n",
    "    transport_df_encoded_test[['transport_index_equally_important_score']]\n",
    ")\n",
    "\n",
    "# Compute the final transport index for both train and test sets\n",
    "def compute_transport_index(group):\n",
    "    # Sort by transport index score and select the top 5 nearest stops\n",
    "    nearest_stops = group.sort_values(by='transport_index_equally_important_score', ascending=False).head(5)\n",
    "    # Sum the transport scores for the top 5 nearest stops\n",
    "    transport_index = np.sum(nearest_stops['transport_index_equally_important_score'])\n",
    "    return transport_index\n",
    "\n",
    "# Apply the function for train and test sets\n",
    "transport_index_train = transport_df_encoded_train.groupby('id').apply(lambda group: compute_transport_index(group))\n",
    "transport_index_test = transport_df_encoded_test.groupby('id').apply(lambda group: compute_transport_index(group))\n",
    "\n",
    "# Assign the transportation index back to the DataFrame\n",
    "transport_df_encoded_train['transport_index'] = transport_df_encoded_train['id'].map(transport_index_train)\n",
    "transport_df_encoded_test['transport_index'] = transport_df_encoded_test['id'].map(transport_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure transport index has the correct name for merging\n",
    "transport_index_train.name = 'transportIndex'\n",
    "transport_index_test.name = 'transportIndex'\n",
    "\n",
    "# Step 2: Merge the transportation index with the existing rental_df_w_education_index_train DataFrame\n",
    "rental_df_train_w_transportIndex = pd.merge(\n",
    "    rental_df_train_nomiss,  # the dataset that already contains the education index\n",
    "    transport_index_train,  # the calculated transport index for the train set\n",
    "    left_on='id',  # merging on 'id'\n",
    "    right_index=True,  # the transportation index has 'id' as the index\n",
    "    how='inner'  # use 'inner' to keep only the records that exist in both datasets\n",
    ")\n",
    "\n",
    "# Step 3: Merge the transportation index with the existing rental_df_w_education_index_test DataFrame\n",
    "rental_df_test_w_transportIndex = pd.merge(\n",
    "    rental_df_test_nomiss,  # the dataset that already contains the education index\n",
    "    transport_index_test,  # the calculated transport index for the test set\n",
    "    left_on='id',  # merging on 'id'\n",
    "    right_index=True,  # the transportation index has 'id' as the index\n",
    "    how='inner'  # use 'inner' to keep only the records that exist in both datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Life Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Caculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"../../data/external/VPA_Open_Space-shp\"\n",
    "\n",
    "open_space_df = gpd.read_file(shapefile_path)\n",
    "\n",
    "rental_df_w_geometry = pd.read_csv('../../data/distance/rental_df_w_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'OS_CATEGOR': ['Cemeteries', 'Civic squares and promenades', 'Conservation reserves', 'Government schools', 'Natural and semi-natural open space', 'Non-government schools', 'Parks and gardens', 'Public housing reserves', 'Recreation corridor', 'Services and utilities reserves', 'Sportsfields and organised recreation', 'Tertiary institutions', 'Transport reservations']\n",
      "Unique values in 'OS_STATUS': ['Existing', 'Planned']\n",
      "Unique values in 'OS_ACCESS': ['Closed', 'Highly Limited', 'Limited', 'Open']\n",
      "Unique values in 'MANAGER_TY': ['Committee - local government', 'Committee - other', 'Crown', 'Local government', 'NO DATA', 'Private', 'Public authority', 'State Government']\n",
      "Unique values in 'OS_TYPE': ['Private open space', 'Public open space', 'Restricted public land']\n",
      "Range of 'SHAPE_Area': (0.0, 0.027314988415512)\n"
     ]
    }
   ],
   "source": [
    "# List of relevant features for open space data\n",
    "relevant_features = [\n",
    "    'FID',               # Unique identifier\n",
    "    'OS_CATEGOR',        # Category of open space      \n",
    "    'OS_STATUS',         # Status of the open space (existing, proposed)\n",
    "    'OS_ACCESS',         # Accessibility information\n",
    "    'MANAGER_TY',        # Management type of the open space\n",
    "    'OS_TYPE',           # Specific type of open space\n",
    "    'SHAPE_Area'         # Area of the open space\n",
    "]\n",
    "\n",
    "# Print unique values for categorical features in order\n",
    "for feature in relevant_features:\n",
    "    if feature not in ['FID', 'SHAPE_Area']:  # Exclude identifier and area feature\n",
    "        unique_values = open_space_df[feature].unique()\n",
    "        print(f\"Unique values in '{feature}': {sorted(unique_values)}\")\n",
    "\n",
    "shape_area_range = (open_space_df['SHAPE_Area'].min(), open_space_df['SHAPE_Area'].max())\n",
    "print(f\"Range of 'SHAPE_Area': {shape_area_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"NO DATA\" with NaN in 'MANAGER_TY'\n",
    "open_space_df['MANAGER_TY'] = open_space_df['MANAGER_TY'].replace('NO DATA', pd.NA)\n",
    "\n",
    "# Fill missing values in 'MANAGER_TY' with the mode\n",
    "open_space_df['MANAGER_TY'].fillna(open_space_df['MANAGER_TY'].mode()[0], inplace=True)\n",
    "\n",
    "# Calculate the mean of 'SHAPE_Area' where the area is greater than 0\n",
    "mean_shape_area = open_space_df.loc[open_space_df['SHAPE_Area'] > 0, 'SHAPE_Area'].mean()\n",
    "\n",
    "# Replace 0.0 values in 'SHAPE_Area' with the mean value\n",
    "open_space_df['SHAPE_Area'] = open_space_df['SHAPE_Area'].replace(0.0, mean_shape_area)\n",
    "\n",
    "# Convert the DataFrame to EPSG:3857 coordinate reference system for geospatial analysis\n",
    "open_spaces_gdf = open_space_df.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_gdf = rental_df_w_geometry.copy()\n",
    "\n",
    "# Create the geometry column from latitude and longitude\n",
    "rental_gdf['geometry'] = rental_gdf.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "rental = gpd.GeoDataFrame(rental_gdf, geometry='geometry', crs=\"EPSG:4326\").to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_open_spaces(property_gdf, open_spaces_gdf, n=5):\n",
    "    expanded_data = []\n",
    "    \n",
    "    # For each property, calculate the distance to all open spaces at once (vectorized)\n",
    "    for idx, property_row in property_gdf.iterrows():\n",
    "        property_geom = property_row['geometry']\n",
    "        \n",
    "        # Calculate distances to all open spaces in one go\n",
    "        open_spaces_gdf['distance'] = open_spaces_gdf['geometry'].distance(property_geom)\n",
    "        \n",
    "        # Filter out invalid distances (NaN or infinity)\n",
    "        valid_open_spaces = open_spaces_gdf[open_spaces_gdf['distance'].notna() & (open_spaces_gdf['distance'] != float('inf'))]\n",
    "        \n",
    "        # Sort the open spaces by distance and select the top n nearest ones\n",
    "        nearest_open_spaces = valid_open_spaces.nsmallest(n, 'distance')\n",
    "        \n",
    "        # Append nearest open spaces to the results or fill with None if fewer than n are found\n",
    "        for i in range(n):\n",
    "            if i < len(nearest_open_spaces):\n",
    "                open_space_row = nearest_open_spaces.iloc[i]\n",
    "                expanded_data.append({\n",
    "                    'property_id': property_row['id'],\n",
    "                    'rent': property_row['rent'],\n",
    "                    'nearest_open_space_id': open_space_row.get('FID', None),\n",
    "                    'nearest_open_space_distance': open_space_row['distance'],\n",
    "                    'OS_CATEGOR': open_space_row['OS_CATEGOR'],\n",
    "                    'OS_STATUS': open_space_row['OS_STATUS'],\n",
    "                    'OS_ACCESS': open_space_row['OS_ACCESS'],\n",
    "                    'MANAGER_TY': open_space_row['MANAGER_TY'],\n",
    "                    'OS_TYPE': open_space_row['OS_TYPE'],\n",
    "                    'SHAPE_Area': open_space_row['SHAPE_Area']\n",
    "                })\n",
    "            else:\n",
    "                # If fewer than n open spaces, append None values\n",
    "                expanded_data.append({\n",
    "                    'property_id': property_row['id'],\n",
    "                    'rent': property_row['rent'],\n",
    "                    'nearest_open_space_id': None,\n",
    "                    'nearest_open_space_distance': None,\n",
    "                    'OS_CATEGOR': None,\n",
    "                    'OS_STATUS': None,\n",
    "                    'OS_ACCESS': None,\n",
    "                    'MANAGER_TY': None,\n",
    "                    'OS_TYPE': None,\n",
    "                    'SHAPE_Area': None\n",
    "                })\n",
    "    \n",
    "    return expanded_data\n",
    "\n",
    "# Call the function with the rental and open spaces GeoDataFrames\n",
    "nearest_open_spaces_data = find_nearest_open_spaces(rental, open_spaces_gdf)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "nearest_open_spaces_df = pd.DataFrame(nearest_open_spaces_data)\n",
    "\n",
    "# Handle missing distances by calculating property-level average distance\n",
    "property_avg_distances = nearest_open_spaces_df.groupby('property_id')['nearest_open_space_distance'].mean()\n",
    "\n",
    "# Fill missing distances with the average distance for the respective property\n",
    "nearest_open_spaces_df['nearest_open_space_distance'].fillna(\n",
    "    nearest_open_spaces_df['property_id'].map(property_avg_distances), inplace=True\n",
    ")\n",
    "\n",
    "#nearest_open_spaces_df.to_csv('../../data/distance/to_openSpace_distance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caculating Life Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_open_spaces_df = pd.read_csv('../../data/distance/to_openSpace_distance.csv')\n",
    "nearest_open_spaces_df = nearest_open_spaces_df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/1641950942.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_open_spaces_train['nearest_open_space_distance'].fillna(dataset_avg_distance_train, inplace=True)\n",
      "/var/folders/gj/fk25jcvs6s927sds6zlbbsjc0000gn/T/ipykernel_6943/1641950942.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nearest_open_spaces_test['nearest_open_space_distance'].fillna(dataset_avg_distance_test, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the dataset by train and test IDs\n",
    "train_ids = rental_df_train_nomiss['id'].unique()\n",
    "test_ids = rental_df_test_nomiss['id'].unique()\n",
    "\n",
    "nearest_open_spaces_train = nearest_open_spaces_df[nearest_open_spaces_df['property_id'].isin(train_ids)]\n",
    "nearest_open_spaces_test = nearest_open_spaces_df[nearest_open_spaces_df['property_id'].isin(test_ids)]\n",
    "\n",
    "# Step 2: Fill missing distances with the dataset-wide average distance for train and test sets\n",
    "dataset_avg_distance_train = nearest_open_spaces_train['nearest_open_space_distance'].mean()\n",
    "dataset_avg_distance_test = nearest_open_spaces_test['nearest_open_space_distance'].mean()\n",
    "\n",
    "nearest_open_spaces_train['nearest_open_space_distance'].fillna(dataset_avg_distance_train, inplace=True)\n",
    "nearest_open_spaces_test['nearest_open_space_distance'].fillna(dataset_avg_distance_test, inplace=True)\n",
    "\n",
    "# Step 3: One-Hot Encoding for train and test sets\n",
    "open_space_related_categorical = ['OS_CATEGOR', 'OS_STATUS', 'OS_ACCESS', 'MANAGER_TY', 'OS_TYPE']\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "open_space_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform on the train dataset\n",
    "open_space_encoded_train = open_space_encoder.fit_transform(nearest_open_spaces_train[open_space_related_categorical])\n",
    "\n",
    "# Transform the test dataset using the encoder fitted on the train dataset\n",
    "open_space_encoded_test = open_space_encoder.transform(nearest_open_spaces_test[open_space_related_categorical])\n",
    "\n",
    "# Get encoded feature names\n",
    "encoded_open_space_feature_names = open_space_encoder.get_feature_names_out(open_space_related_categorical)\n",
    "\n",
    "# Step 4: Create DataFrames for train and test sets with encoded features\n",
    "open_space_df_encoded_train = pd.DataFrame(open_space_encoded_train, columns=encoded_open_space_feature_names)\n",
    "open_space_df_encoded_train['property_id'] = nearest_open_spaces_train['property_id'].values\n",
    "open_space_df_encoded_train['rent'] = nearest_open_spaces_train['rent'].values\n",
    "open_space_df_encoded_train['nearest_open_space_distance'] = nearest_open_spaces_train['nearest_open_space_distance'].values\n",
    "\n",
    "open_space_df_encoded_test = pd.DataFrame(open_space_encoded_test, columns=encoded_open_space_feature_names)\n",
    "open_space_df_encoded_test['property_id'] = nearest_open_spaces_test['property_id'].values\n",
    "open_space_df_encoded_test['rent'] = nearest_open_spaces_test['rent'].values\n",
    "open_space_df_encoded_test['nearest_open_space_distance'] = nearest_open_spaces_test['nearest_open_space_distance'].values\n",
    "\n",
    "# Step 5: Min-Max Scaling on encoded features and nearest_open_space_distance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the encoded categorical features and distance separately\n",
    "scaled_features_train = scaler.fit_transform(pd.concat([open_space_df_encoded_train[encoded_open_space_feature_names],\n",
    "                                                        open_space_df_encoded_train[['nearest_open_space_distance']]], axis=1))\n",
    "\n",
    "scaled_features_test = scaler.transform(pd.concat([open_space_df_encoded_test[encoded_open_space_feature_names],\n",
    "                                                   open_space_df_encoded_test[['nearest_open_space_distance']]], axis=1))\n",
    "\n",
    "# Ensure the scaled features have the correct column names\n",
    "scaled_feature_names_train = encoded_open_space_feature_names.tolist() + ['nearest_open_space_distance']\n",
    "scaled_feature_names_test = encoded_open_space_feature_names.tolist() + ['nearest_open_space_distance']\n",
    "\n",
    "# Update the DataFrames with scaled features\n",
    "open_space_df_encoded_train[scaled_feature_names_train] = scaled_features_train\n",
    "open_space_df_encoded_test[scaled_feature_names_test] = scaled_features_test\n",
    "\n",
    "# Step 6: Define feature matrix (X) and target variable (y) for train and test sets\n",
    "X_open_space_train = open_space_df_encoded_train[scaled_feature_names_train]\n",
    "y_open_space_train = open_space_df_encoded_train['rent']\n",
    "\n",
    "X_open_space_test = open_space_df_encoded_test[scaled_feature_names_test]\n",
    "y_open_space_test = open_space_df_encoded_test['rent']\n",
    "\n",
    "# Step 7: Cross-validation with Random Forest on the train set\n",
    "cv_open_space = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_model_open_space = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "open_space_score_train = cross_val_predict(rf_model_open_space, X_open_space_train, y_open_space_train, cv=cv_open_space)\n",
    "\n",
    "# Fit the model on the full train set and predict on the test set\n",
    "rf_model_open_space.fit(X_open_space_train, y_open_space_train)\n",
    "open_space_score_test = rf_model_open_space.predict(X_open_space_test)\n",
    "\n",
    "# Step 8: Normalize Open Space Scores for both train and test sets\n",
    "scaler_open_space = MinMaxScaler()\n",
    "\n",
    "# Train set\n",
    "open_space_score_reshaped_train = open_space_score_train.reshape(-1, 1)\n",
    "open_space_score_normalized_train = scaler_open_space.fit_transform(open_space_score_reshaped_train)\n",
    "\n",
    "# Test set\n",
    "open_space_score_reshaped_test = open_space_score_test.reshape(-1, 1)\n",
    "open_space_score_normalized_test = scaler_open_space.transform(open_space_score_reshaped_test)\n",
    "\n",
    "# Assign normalized scores to train and test DataFrames\n",
    "open_space_df_encoded_train['open_space_score'] = open_space_score_normalized_train\n",
    "open_space_df_encoded_test['open_space_score'] = open_space_score_normalized_test\n",
    "\n",
    "# Step 9: Normalize reverse distances for both train and test sets\n",
    "distance_scaler_open_space = MinMaxScaler()\n",
    "\n",
    "# Train set\n",
    "open_space_df_encoded_train['reverse_distance/m'] = 1 - distance_scaler_open_space.fit_transform(\n",
    "    open_space_df_encoded_train[['nearest_open_space_distance']]\n",
    ")\n",
    "\n",
    "# Test set\n",
    "open_space_df_encoded_test['reverse_distance/m'] = 1 - distance_scaler_open_space.transform(\n",
    "    open_space_df_encoded_test[['nearest_open_space_distance']]\n",
    ")\n",
    "\n",
    "# Step 10: Calculate Open Space Index for both train and test sets\n",
    "open_space_df_encoded_train['open_space_index_equally_important_score'] = (\n",
    "    0.5 * open_space_df_encoded_train['open_space_score'] + 0.5 * open_space_df_encoded_train['reverse_distance/m']\n",
    ")\n",
    "\n",
    "open_space_df_encoded_test['open_space_index_equally_important_score'] = (\n",
    "    0.5 * open_space_df_encoded_test['open_space_score'] + 0.5 * open_space_df_encoded_test['reverse_distance/m']\n",
    ")\n",
    "\n",
    "# Step 11: Normalize the final Open Space Index for both train and test sets\n",
    "scaler_open_space_final = MinMaxScaler()\n",
    "\n",
    "# Train set\n",
    "open_space_df_encoded_train['open_space_index_equally_important_score'] = scaler_open_space_final.fit_transform(\n",
    "    open_space_df_encoded_train[['open_space_index_equally_important_score']]\n",
    ")\n",
    "\n",
    "# Test set\n",
    "open_space_df_encoded_test['open_space_index_equally_important_score'] = scaler_open_space_final.transform(\n",
    "    open_space_df_encoded_test[['open_space_index_equally_important_score']]\n",
    ")\n",
    "\n",
    "# Step 12: Compute the final Open Space Index for both train and test sets\n",
    "def compute_open_space_index(group):\n",
    "    # Sort by open space index score and select the top 5 nearest spaces\n",
    "    nearest_open_spaces = group.sort_values(by='open_space_index_equally_important_score', ascending=False).head(5)\n",
    "    # Sum the open space scores for the top 5 nearest spaces\n",
    "    open_space_index = np.sum(nearest_open_spaces['open_space_index_equally_important_score'])\n",
    "    return open_space_index\n",
    "\n",
    "# Apply the function for train and test sets\n",
    "open_space_index_train = open_space_df_encoded_train.groupby('property_id').apply(lambda group: compute_open_space_index(group))\n",
    "open_space_index_test = open_space_df_encoded_test.groupby('property_id').apply(lambda group: compute_open_space_index(group))\n",
    "\n",
    "# Step 13: Assign the Open Space Index back to the DataFrame\n",
    "open_space_df_encoded_train['open_space_index'] = open_space_df_encoded_train['property_id'].map(open_space_index_train)\n",
    "open_space_df_encoded_test['open_space_index'] = open_space_df_encoded_test['property_id'].map(open_space_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the open space index\n",
    "def compute_open_space_index(group):\n",
    "    # Sort by reverse distance (closer open spaces first) and select the top 5 nearest spaces\n",
    "    nearest_open_spaces = group.sort_values(by='open_space_index_equally_important_score', ascending=False).head(5)\n",
    "    \n",
    "    # Sum the open space scores for the top 5 nearest open spaces\n",
    "    open_space_index = np.sum(nearest_open_spaces['open_space_index_equally_important_score'])\n",
    "    \n",
    "    return open_space_index\n",
    "\n",
    "# Apply this function to compute the open space index for the train set\n",
    "open_space_index_train = open_space_df_encoded_train.groupby('property_id').apply(lambda group: compute_open_space_index(group))\n",
    "\n",
    "# Apply this function to compute the open space index for the test set\n",
    "open_space_index_test = open_space_df_encoded_test.groupby('property_id').apply(lambda group: compute_open_space_index(group))\n",
    "\n",
    "# Ensure that the column has the correct name for merging\n",
    "open_space_index_train.name = 'lifeIndex'\n",
    "open_space_index_test.name = 'lifeIndex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_w_index_train = pd.merge(\n",
    "    rental_df_train_w_transportIndex,  # The dataset that already contains the education and transport index\n",
    "    open_space_index_train.rename('lifeIndex'),  # The open space index, renamed to 'lifeIndex'\n",
    "    left_on='id',  # Merging on 'id' in the rental dataset\n",
    "    right_index=True,  # Merging on the index from the open space index (property_id is the index)\n",
    "    how='inner'  # Perform an inner join to keep only matching records\n",
    ")\n",
    "\n",
    "# Step 2: Merge the open space index into the rental_df_w_education_transport_index_test\n",
    "rental_df_w_index_test = pd.merge(\n",
    "    rental_df_test_w_transportIndex,  # The dataset that already contains the education and transport index\n",
    "    open_space_index_test.rename('lifeIndex'),  # The open space index, renamed to 'lifeIndex'\n",
    "    left_on='id',  # Merging on 'id' in the rental dataset\n",
    "    right_index=True,  # Merging on the index from the open space index (property_id is the index)\n",
    "    how='inner'  # Perform an inner join to keep only matching records\n",
    ")\n",
    "\n",
    "# Step 3: Drop the 'order' column if it's not needed in both train and test datasets\n",
    "rental_df_w_index_train.drop(columns='order', inplace=True, errors='ignore')\n",
    "rental_df_w_index_test.drop(columns='order', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_w_index_train = rental_df_w_index_train.drop(columns='address')\n",
    "rental_df_w_index_test = rental_df_w_index_test.drop(columns='address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Part 1) Feature Engineering on physical characteristics of property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- property type (one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the column index of 'propertyType' in both train and test datasets\n",
    "col_index_train = rental_df_w_index_train.columns.get_loc('propertyType')\n",
    "\n",
    "# Step 2: Apply get_dummies to 'propertyType' for the train dataset\n",
    "rental_df_numericType_train = pd.get_dummies(rental_df_w_index_train, columns=['propertyType'], prefix='propertyType')\n",
    "\n",
    "# Apply get_dummies to 'propertyType' for the test dataset\n",
    "rental_df_numericType_test = pd.get_dummies(rental_df_w_index_test, columns=['propertyType'], prefix='propertyType')\n",
    "\n",
    "# Step 3: Align the columns of the test dataset with the train dataset\n",
    "# Ensure the test set has the same columns as the train set\n",
    "rental_df_numericType_test = rental_df_numericType_test.reindex(columns=rental_df_numericType_train.columns, fill_value=0)\n",
    "\n",
    "# Step 4: Reorder the columns in the train dataset to maintain the position of the encoded propertyType\n",
    "property_type_columns_train = [col for col in rental_df_numericType_train.columns if col.startswith('propertyType_')]\n",
    "columns_train = rental_df_numericType_train.columns.tolist()\n",
    "\n",
    "for col in property_type_columns_train:\n",
    "    columns_train.remove(col)\n",
    "columns_train[col_index_train:col_index_train] = property_type_columns_train\n",
    "\n",
    "rental_df_numericType_train = rental_df_numericType_train[columns_train]\n",
    "\n",
    "# Step 5: Reorder the columns in the test dataset to maintain the position of the encoded propertyType\n",
    "property_type_columns_test = [col for col in rental_df_numericType_test.columns if col.startswith('propertyType_')]\n",
    "columns_test = rental_df_numericType_test.columns.tolist()\n",
    "\n",
    "for col in property_type_columns_test:\n",
    "    columns_test.remove(col)\n",
    "columns_test[col_index_train:col_index_train] = property_type_columns_test  # Use col_index_train to align with the train set\n",
    "\n",
    "rental_df_numericType_test = rental_df_numericType_test[columns_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- property feature (frequency encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_numericFeatures_train = rental_df_numericType_train.copy()\n",
    "rental_df_numericFeatures_test = rental_df_numericType_test.copy()\n",
    "\n",
    "# Split the 'propertyFeatures' column in both datasets\n",
    "rental_df_numericFeatures_train['propertyFeatures'] = rental_df_numericFeatures_train['propertyFeatures'].str.split(',')\n",
    "rental_df_numericFeatures_test['propertyFeatures'] = rental_df_numericFeatures_test['propertyFeatures'].str.split(',')\n",
    "\n",
    "# Find all unique features in the train dataset\n",
    "all_features_train = [feature.strip() for sublist in rental_df_numericFeatures_train['propertyFeatures'] for feature in sublist]\n",
    "\n",
    "# Create feature counts for train data and calculate log-inverse feature mapping\n",
    "feature_counts_train = pd.Series(all_features_train).value_counts()\n",
    "log_inverse_feature_mapping_train = {feature: 1 / np.log(count + 1) for feature, count in feature_counts_train.items()}\n",
    "\n",
    "# Calculate mean log-inverse score for train data\n",
    "mean_log_inverse_score_train = pd.Series(list(log_inverse_feature_mapping_train.values())).mean()\n",
    "\n",
    "# Define a function to encode features based on the log-inverse frequency\n",
    "def encode_features(features, feature_mapping, mean_score):\n",
    "    if features == ['unknown']:\n",
    "        return mean_score  # Use mean inverse score for 'unknown'\n",
    "    return sum([feature_mapping.get(f.strip(), 0) for f in features])\n",
    "\n",
    "# Apply encoding to the 'propertyFeatures' column for train dataset\n",
    "rental_df_numericFeatures_train['propertyFeatures_encoded'] = rental_df_numericFeatures_train['propertyFeatures'].apply(\n",
    "    lambda features: encode_features(features, log_inverse_feature_mapping_train, mean_log_inverse_score_train)\n",
    ")\n",
    "\n",
    "# Apply encoding to the 'propertyFeatures' column for test dataset (using train mapping)\n",
    "rental_df_numericFeatures_test['propertyFeatures_encoded'] = rental_df_numericFeatures_test['propertyFeatures'].apply(\n",
    "    lambda features: encode_features(features, log_inverse_feature_mapping_train, mean_log_inverse_score_train)\n",
    ")\n",
    "\n",
    "#  Reorder the columns to place 'propertyFeatures_encoded' in the original 'propertyFeatures' position for both train and test datasets\n",
    "col_index_train = rental_df_numericFeatures_train.columns.get_loc('propertyFeatures')\n",
    "col_index_test = rental_df_numericFeatures_test.columns.get_loc('propertyFeatures')\n",
    "\n",
    "columns_train = rental_df_numericFeatures_train.columns.tolist()\n",
    "columns_train.insert(col_index_train, columns_train.pop(columns_train.index('propertyFeatures_encoded')))\n",
    "\n",
    "columns_test = rental_df_numericFeatures_test.columns.tolist()\n",
    "columns_test.insert(col_index_test, columns_test.pop(columns_test.index('propertyFeatures_encoded')))\n",
    "\n",
    "rental_df_numericFeatures_train = rental_df_numericFeatures_train[columns_train]\n",
    "rental_df_numericFeatures_test = rental_df_numericFeatures_test[columns_test]\n",
    "\n",
    "# Step 10: Drop the original 'propertyFeatures' column from both train and test datasets\n",
    "rental_df_numericFeatures_train = rental_df_numericFeatures_train.drop(columns=['propertyFeatures'])\n",
    "rental_df_numericFeatures_test = rental_df_numericFeatures_test.drop(columns=['propertyFeatures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- description (Doc2Vec embeddings than PCA reduce dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Copy train and test datasets\n",
    "rental_df_numericDescription_train = rental_df_numericFeatures_train.copy()\n",
    "rental_df_numericDescription_test = rental_df_numericFeatures_test.copy()\n",
    "\n",
    "# Step 2: Reset the index to ensure a clean, continuous sequence\n",
    "rental_df_numericDescription_train = rental_df_numericDescription_train.reset_index(drop=True)\n",
    "rental_df_numericDescription_test = rental_df_numericDescription_test.reset_index(drop=True)\n",
    "\n",
    "# Step 3: Ensure that no descriptions are missing or empty in the train and test datasets\n",
    "rental_df_numericDescription_train = rental_df_numericDescription_train[\n",
    "    rental_df_numericDescription_train['description'].notna() & \n",
    "    (rental_df_numericDescription_train['description'].str.strip() != \"\")\n",
    "]\n",
    "rental_df_numericDescription_test = rental_df_numericDescription_test[\n",
    "    rental_df_numericDescription_test['description'].notna() & \n",
    "    (rental_df_numericDescription_test['description'].str.strip() != \"\")\n",
    "]\n",
    "\n",
    "# Step 4: Prepare tagged data for training the Doc2Vec model using the train dataset\n",
    "# Use the newly reset DataFrame index as tags to ensure matching tags and indices\n",
    "tagged_data_train = [\n",
    "    TaggedDocument(words=desc.split(), tags=[str(i)]) \n",
    "    for i, desc in zip(rental_df_numericDescription_train.index, rental_df_numericDescription_train['description'])\n",
    "]\n",
    "\n",
    "# Step 5: Train the Doc2Vec model on the train dataset descriptions\n",
    "doc2vec_model = Doc2Vec(tagged_data_train, vector_size=300, window=5, min_count=1, workers=4, epochs=40)\n",
    "\n",
    "# Save the model if needed\n",
    "doc2vec_model.save(\"../../models/doc2vec_rental_descriptions_train.model\")\n",
    "\n",
    "# Step 6: Retrieve the Doc2Vec embeddings for the train dataset using matching index values\n",
    "def get_doc2vec_embedding(index):\n",
    "    try:\n",
    "        return doc2vec_model.dv.get_vector(str(index))  # Use get_vector instead of get\n",
    "    except KeyError:\n",
    "        return np.zeros(doc2vec_model.vector_size)  # Return zero vector for missing keys\n",
    "\n",
    "# Apply the function to retrieve embeddings for the train dataset\n",
    "rental_df_numericDescription_train['doc2vec_embeddings'] = rental_df_numericDescription_train.index.to_series().apply(get_doc2vec_embedding)\n",
    "\n",
    "# Step 7: Infer the embeddings for the test dataset\n",
    "rental_df_numericDescription_test['doc2vec_embeddings'] = rental_df_numericDescription_test['description'].apply(\n",
    "    lambda x: doc2vec_model.infer_vector(x.split())  # Infer embeddings for each description in the test dataset\n",
    ")\n",
    "\n",
    "# Step 8: Combine embeddings into arrays for PCA transformation\n",
    "embeddings_train = np.vstack(rental_df_numericDescription_train['doc2vec_embeddings'].values)\n",
    "embeddings_test = np.vstack(rental_df_numericDescription_test['doc2vec_embeddings'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Apply PCA to reduce the dimensionality of embeddings (fit on train, transform test)\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# PCA on train data\n",
    "reduced_embeddings_train = pca.fit_transform(embeddings_train)\n",
    "\n",
    "# PCA on test data (using the same PCA model trained on train data)\n",
    "reduced_embeddings_test = pca.transform(embeddings_test)\n",
    "\n",
    "# Step 10: Convert reduced embeddings into DataFrames\n",
    "reduced_embeddings_train_df = pd.DataFrame(reduced_embeddings_train, columns=[f'doc2vec_embedding_{i+1}' for i in range(reduced_embeddings_train.shape[1])])\n",
    "reduced_embeddings_test_df = pd.DataFrame(reduced_embeddings_test, columns=[f'doc2vec_embedding_{i+1}' for i in range(reduced_embeddings_test.shape[1])])\n",
    "\n",
    "# Step 11: Find the position of the 'description' column\n",
    "col_index_train = rental_df_numericDescription_train.columns.get_loc('description')\n",
    "col_index_test = rental_df_numericDescription_test.columns.get_loc('description')\n",
    "\n",
    "# Step 12: Insert reduced embeddings right after the original 'description' column\n",
    "for i, col in enumerate(reduced_embeddings_train_df.columns):\n",
    "    rental_df_numericDescription_train.insert(col_index_train + 1 + i, col, reduced_embeddings_train_df[col])\n",
    "\n",
    "for i, col in enumerate(reduced_embeddings_test_df.columns):\n",
    "    rental_df_numericDescription_test.insert(col_index_test + 1 + i, col, reduced_embeddings_test_df[col])\n",
    "\n",
    "# Step 13: Drop the original 'description' column from both datasets\n",
    "rental_df_numericDescription_train = rental_df_numericDescription_train.drop(columns=['description'])\n",
    "rental_df_numericDescription_test = rental_df_numericDescription_test.drop(columns=['description'])\n",
    "\n",
    "# Step 14: Drop the 'doc2vec_embeddings' column if present in both datasets\n",
    "if 'doc2vec_embeddings' in rental_df_numericDescription_train.columns:\n",
    "    rental_df_numericDescription_train = rental_df_numericDescription_train.drop(columns=['doc2vec_embeddings'])\n",
    "if 'doc2vec_embeddings' in rental_df_numericDescription_test.columns:\n",
    "    rental_df_numericDescription_test = rental_df_numericDescription_test.drop(columns=['doc2vec_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df_numericDescription_train.to_csv('../../data/curated/rental_df_numericDescription_train.csv')\n",
    "rental_df_numericDescription_test.to_csv('../../data/curated/rental_df_numericDescription_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rent</th>\n",
       "      <th>suburb_encoded</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>parking</th>\n",
       "      <th>propertyType_Apartment</th>\n",
       "      <th>propertyType_Block of Units</th>\n",
       "      <th>propertyType_Duplex</th>\n",
       "      <th>propertyType_House</th>\n",
       "      <th>...</th>\n",
       "      <th>doc2vec_embedding_4</th>\n",
       "      <th>doc2vec_embedding_5</th>\n",
       "      <th>doc2vec_embedding_6</th>\n",
       "      <th>doc2vec_embedding_7</th>\n",
       "      <th>doc2vec_embedding_8</th>\n",
       "      <th>doc2vec_embedding_9</th>\n",
       "      <th>doc2vec_embedding_10</th>\n",
       "      <th>educationIndex</th>\n",
       "      <th>transportIndex</th>\n",
       "      <th>lifeIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17205306</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>562.156107</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836285</td>\n",
       "      <td>0.036708</td>\n",
       "      <td>-1.523336</td>\n",
       "      <td>0.145149</td>\n",
       "      <td>0.608593</td>\n",
       "      <td>-0.013410</td>\n",
       "      <td>-0.449576</td>\n",
       "      <td>3.336283</td>\n",
       "      <td>3.632117</td>\n",
       "      <td>3.724052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17198219</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>481.428571</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319007</td>\n",
       "      <td>-0.185621</td>\n",
       "      <td>-0.046673</td>\n",
       "      <td>-0.164099</td>\n",
       "      <td>-0.342121</td>\n",
       "      <td>0.692183</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>3.336073</td>\n",
       "      <td>2.803054</td>\n",
       "      <td>2.730468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17192805</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>415.384615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311565</td>\n",
       "      <td>0.183104</td>\n",
       "      <td>0.645433</td>\n",
       "      <td>0.586536</td>\n",
       "      <td>0.745326</td>\n",
       "      <td>-0.234981</td>\n",
       "      <td>-1.923859</td>\n",
       "      <td>3.316745</td>\n",
       "      <td>2.946476</td>\n",
       "      <td>3.063298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17186367</td>\n",
       "      <td>601.020209</td>\n",
       "      <td>613.008533</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>-0.007321</td>\n",
       "      <td>-0.536598</td>\n",
       "      <td>0.301277</td>\n",
       "      <td>-0.902802</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>0.174813</td>\n",
       "      <td>3.753792</td>\n",
       "      <td>3.842829</td>\n",
       "      <td>3.758799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17183641</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>496.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602301</td>\n",
       "      <td>0.116313</td>\n",
       "      <td>-0.873118</td>\n",
       "      <td>-0.532432</td>\n",
       "      <td>-0.095673</td>\n",
       "      <td>-0.477763</td>\n",
       "      <td>-0.348878</td>\n",
       "      <td>3.540573</td>\n",
       "      <td>2.889705</td>\n",
       "      <td>3.536137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>17074068</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311331</td>\n",
       "      <td>-0.441705</td>\n",
       "      <td>0.215627</td>\n",
       "      <td>-0.055897</td>\n",
       "      <td>-0.562288</td>\n",
       "      <td>-0.073010</td>\n",
       "      <td>-0.582896</td>\n",
       "      <td>3.679646</td>\n",
       "      <td>3.621857</td>\n",
       "      <td>3.774974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>17203930</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>492.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331228</td>\n",
       "      <td>-0.631301</td>\n",
       "      <td>0.657347</td>\n",
       "      <td>0.227995</td>\n",
       "      <td>-0.444900</td>\n",
       "      <td>-1.123487</td>\n",
       "      <td>0.384566</td>\n",
       "      <td>3.221527</td>\n",
       "      <td>0.695847</td>\n",
       "      <td>0.636994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>17199166</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>641.190476</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221875</td>\n",
       "      <td>-0.951697</td>\n",
       "      <td>-1.025178</td>\n",
       "      <td>-0.192776</td>\n",
       "      <td>0.327907</td>\n",
       "      <td>-0.679706</td>\n",
       "      <td>-0.596598</td>\n",
       "      <td>3.544625</td>\n",
       "      <td>3.068617</td>\n",
       "      <td>3.619969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>17198474</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697269</td>\n",
       "      <td>0.423407</td>\n",
       "      <td>-0.712353</td>\n",
       "      <td>0.186623</td>\n",
       "      <td>-1.073208</td>\n",
       "      <td>-0.209014</td>\n",
       "      <td>0.942093</td>\n",
       "      <td>1.262079</td>\n",
       "      <td>2.757537</td>\n",
       "      <td>3.702783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>17172481</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>581.136028</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268337</td>\n",
       "      <td>-0.991041</td>\n",
       "      <td>-0.861633</td>\n",
       "      <td>-0.967453</td>\n",
       "      <td>-0.123615</td>\n",
       "      <td>-0.338303</td>\n",
       "      <td>-1.057173</td>\n",
       "      <td>3.569685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.401100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1480 rows Ã 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        rent  suburb_encoded  bedroom  bathroom  parking  \\\n",
       "0     17205306  550.000000      562.156107      3.0         2      3.0   \n",
       "1     17198219  470.000000      481.428571      4.0         2      2.0   \n",
       "2     17192805  400.000000      415.384615      3.0         1      3.0   \n",
       "3     17186367  601.020209      613.008533      4.0         4      2.0   \n",
       "4     17183641  470.000000      496.333333      3.0         1      1.0   \n",
       "...        ...         ...             ...      ...       ...      ...   \n",
       "1475  17074068  800.000000      713.000000      3.0         2      2.0   \n",
       "1476  17203930  430.000000      492.500000      3.0         2      2.0   \n",
       "1477  17199166  950.000000      641.190476      4.0         3      1.0   \n",
       "1478  17198474  450.000000      550.000000      3.0         2      2.0   \n",
       "1479  17172481  650.000000      581.136028      2.0         2      1.0   \n",
       "\n",
       "      propertyType_Apartment  propertyType_Block of Units  \\\n",
       "0                          0                            0   \n",
       "1                          0                            0   \n",
       "2                          0                            0   \n",
       "3                          0                            0   \n",
       "4                          0                            0   \n",
       "...                      ...                          ...   \n",
       "1475                       0                            0   \n",
       "1476                       0                            0   \n",
       "1477                       0                            0   \n",
       "1478                       0                            0   \n",
       "1479                       0                            0   \n",
       "\n",
       "      propertyType_Duplex  propertyType_House  ...  doc2vec_embedding_4  \\\n",
       "0                       0                   1  ...             0.836285   \n",
       "1                       0                   1  ...            -0.319007   \n",
       "2                       0                   1  ...             0.311565   \n",
       "3                       0                   1  ...             0.462641   \n",
       "4                       0                   1  ...             0.602301   \n",
       "...                   ...                 ...  ...                  ...   \n",
       "1475                    0                   0  ...             0.311331   \n",
       "1476                    0                   0  ...             1.331228   \n",
       "1477                    0                   0  ...             0.221875   \n",
       "1478                    0                   0  ...             0.697269   \n",
       "1479                    0                   0  ...             0.268337   \n",
       "\n",
       "      doc2vec_embedding_5  doc2vec_embedding_6  doc2vec_embedding_7  \\\n",
       "0                0.036708            -1.523336             0.145149   \n",
       "1               -0.185621            -0.046673            -0.164099   \n",
       "2                0.183104             0.645433             0.586536   \n",
       "3               -0.007321            -0.536598             0.301277   \n",
       "4                0.116313            -0.873118            -0.532432   \n",
       "...                   ...                  ...                  ...   \n",
       "1475            -0.441705             0.215627            -0.055897   \n",
       "1476            -0.631301             0.657347             0.227995   \n",
       "1477            -0.951697            -1.025178            -0.192776   \n",
       "1478             0.423407            -0.712353             0.186623   \n",
       "1479            -0.991041            -0.861633            -0.967453   \n",
       "\n",
       "      doc2vec_embedding_8  doc2vec_embedding_9  doc2vec_embedding_10  \\\n",
       "0                0.608593            -0.013410             -0.449576   \n",
       "1               -0.342121             0.692183              0.000414   \n",
       "2                0.745326            -0.234981             -1.923859   \n",
       "3               -0.902802             0.061945              0.174813   \n",
       "4               -0.095673            -0.477763             -0.348878   \n",
       "...                   ...                  ...                   ...   \n",
       "1475            -0.562288            -0.073010             -0.582896   \n",
       "1476            -0.444900            -1.123487              0.384566   \n",
       "1477             0.327907            -0.679706             -0.596598   \n",
       "1478            -1.073208            -0.209014              0.942093   \n",
       "1479            -0.123615            -0.338303             -1.057173   \n",
       "\n",
       "      educationIndex  transportIndex  lifeIndex  \n",
       "0           3.336283        3.632117   3.724052  \n",
       "1           3.336073        2.803054   2.730468  \n",
       "2           3.316745        2.946476   3.063298  \n",
       "3           3.753792        3.842829   3.758799  \n",
       "4           3.540573        2.889705   3.536137  \n",
       "...              ...             ...        ...  \n",
       "1475        3.679646        3.621857   3.774974  \n",
       "1476        3.221527        0.695847   0.636994  \n",
       "1477        3.544625        3.068617   3.619969  \n",
       "1478        1.262079        2.757537   3.702783  \n",
       "1479        3.569685        0.000000   3.401100  \n",
       "\n",
       "[1480 rows x 37 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rental_df_numericDescription_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
